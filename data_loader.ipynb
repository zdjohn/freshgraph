{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data: HetRec 2011 MovieLens + IMDb/Rotten Tomatoes\n",
    "https://grouplens.org/datasets/hetrec-2011/\n",
    "readme: http://files.grouplens.org/datasets/hetrec2011/hetrec2011-movielens-readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from freshgraph import data_loader\n",
    "from freshgraph.data_loader import user_item_tuple_train, movie_meta, movies_validate, movie_genres, movie_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data into graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "movie_feature_graph = data_loader.get_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: candidating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "## tensorflow warning supression\n",
    "from freshgraph import candidating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create metapath2vec based model for item to item similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: training similarity model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metapath2vec_model = candidating.train_candidates_model(movie_feature_graph)\n",
    "metapath2vec_model.save(\"movie.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 1: load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 271 ms, sys: 52.8 ms, total: 324 ms\n",
      "Wall time: 321 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import Word2Vec\n",
    "metapath2vec_model = Word2Vec.load(\"movie.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_validate[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Star Trek IV: The Voyage Home']\n",
      "0.5979886651039124\n",
      "['Star Trek III: The Search for Spock']\n",
      "0.5717388987541199\n",
      "['Star Trek V: The Final Frontier']\n",
      "0.5665578842163086\n",
      "['Mission: Impossible III']\n",
      "0.5640816688537598\n",
      "['Titanica']\n",
      "0.5558382272720337\n",
      "['Star Trek: The Wrath of Khan']\n",
      "0.5479453206062317\n",
      "['Star Trek VI: The Undiscovered Country']\n",
      "0.5397538542747498\n",
      "['Mission: Impossible III']\n",
      "0.5361419916152954\n",
      "['Destiny in Space']\n",
      "0.5297540426254272\n",
      "['Bottle Shock']\n",
      "0.5221686363220215\n"
     ]
    }
   ],
   "source": [
    "validate_id = 3884 #, Star Trek, 25940\tThe Lady from Shanghai\t\n",
    "\n",
    "similar_items = candidating.get_similar_items(metapath2vec_model, validate_id)\n",
    "ordered_item_list = candidating.sort_similar_item_by_score(similar_items)\n",
    "for x in ordered_item_list[:10]:\n",
    "    print(movie_meta[movie_meta['id'] == int(x[0])]['title'].values)\n",
    "    print(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_users = candidating.get_ranked_candidates(user_item_tuple_train, similar_items)\n",
    "candidate_ids = [x[0] for x in ranked_users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_tuple_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: drift detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create user-items and item-users mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 39s, sys: 36.5 ms, total: 1min 39s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from collections import defaultdict\n",
    "\n",
    "# user-items tuples based on candidate\n",
    "candidate_items_tuple = user_item_tuple_train[user_item_tuple_train['userID'].isin([int(x) for x in candidate_ids])]\n",
    "# candidate_items_tuple = user_item_tuple_train\n",
    "\n",
    "user_items_dict = defaultdict(list)\n",
    "candidate_items_subgraph = [user_items_dict[x['userID']].append(x['movieID']) for _, x in candidate_items_tuple.iterrows()]\n",
    "\n",
    "item_users_dict = defaultdict(list)\n",
    "item_candidates_subgraph = [item_users_dict[x['movieID']].append(x['userID']) for _, x in candidate_items_tuple.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2112\n",
      "9303\n"
     ]
    }
   ],
   "source": [
    "print(len(user_items_dict))\n",
    "print(len(item_users_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create partition on user and item nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "partition_user_dict={}\n",
    "partition_item_dict={}\n",
    "partition_index = 0\n",
    "\n",
    "def partition_vertices(user_id, partition_user_dict, partition_item_dict, partition_index):\n",
    "    partition_user_dict[user_id] = partition_index\n",
    "    grouped_candidate_list = []\n",
    "    for item_id in user_items_dict[user_id]:\n",
    "        if item_id not in partition_item_dict:\n",
    "            partition_item_dict[item_id]=partition_index\n",
    "            grouped_candidate_list += item_users_dict[item_id]\n",
    "#     print(grouped_candidate_list)\n",
    "    for related_user_id in set(grouped_candidate_list):\n",
    "        if related_user_id not in partition_user_dict:\n",
    "            partition_vertices(related_user_id, partition_user_dict, partition_item_dict, partition_index)\n",
    "    \n",
    "\n",
    "for user_id in user_items_dict.keys():\n",
    "    if user_id not in partition_user_dict:\n",
    "        partition_vertices(user_id, partition_user_dict, partition_item_dict, partition_index)\n",
    "        partition_index += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2112"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(partition_user_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staged_edges = []\n",
    "\n",
    "candidate_item_pair = [(int(x[0]), validate_id+100000) for x in ranked_users]\n",
    "\n",
    "i=0\n",
    "while i < len(candidate_item_pair):\n",
    "    \n",
    "    staged_ui = init_user_item_tuple.extend(candidate_item_pair[i:i+10])\n",
    "    staged_edges.append(init_user_item_tuple.copy())\n",
    "    i += 10\n",
    "\n",
    "# [len(x) for x in staged_edges]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# staged_edges[0][:20]\n",
    "# [ (u,v) for u,v in staged_edges[0] if (u <=0 or v <=0)]\n",
    "# from igraph import Graph\n",
    "g = Graph.Bipartite(list(init_types.values())+[1], staged_edges[0], directed=False)\n",
    "# segments = [Graph.Bipartite([1,0], e, directed=False) for e in staged_edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from igraph import Graph\n",
    "g = Graph.Bipartite([0, 1, 0, 1, 1, 0], [(0, 1), (2, 3), (0, 3), (0,4), (2,4), (5,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.es[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i, x in enumerate(g.vs[\"type\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1,0]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "freshgraph",
   "language": "python",
   "name": "freshgraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
